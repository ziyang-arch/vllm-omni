# Stage config for running qwen2.5-omni on 2 GPUs with BATCHING.
# Stage-0 (thinker) on GPU 0, Stage-1 (talker) on GPU 1, Stage-2 (code2wav) on GPU 0.
# Optimized for 2x 48GB GPUs (RTX A6000) with batch_size=4.

stage_args:
  - stage_id: 0
    stage_type: llm
    runtime:
      process: true
      devices: "0"
      max_batch_size: 4
    engine_args:
      model_stage: thinker
      model_arch: Qwen2_5OmniForConditionalGeneration
      worker_cls: vllm_omni.worker.gpu_ar_worker.GPUARWorker
      scheduler_cls: vllm_omni.core.sched.omni_ar_scheduler.OmniARScheduler
      gpu_memory_utilization: 0.85
      max_model_len: 8192
      max_num_seqs: 4
      enforce_eager: true
      trust_remote_code: true
      engine_output_type: latent
      enable_prefix_caching: false
      max_num_batched_tokens: 32768
    is_comprehension: true
    final_output: true
    final_output_type: text
    default_sampling_params:
      temperature: 0.0
      top_p: 1.0
      top_k: -1
      max_tokens: 2048
      seed: 42
      detokenize: True
      repetition_penalty: 1.1

  - stage_id: 1
    stage_type: llm
    runtime:
      process: true
      devices: "1"
      max_batch_size: 4
    engine_args:
      model_stage: talker
      model_arch: Qwen2_5OmniForConditionalGeneration
      worker_cls: vllm_omni.worker.gpu_ar_worker.GPUARWorker
      scheduler_cls: vllm_omni.core.sched.omni_ar_scheduler.OmniARScheduler
      gpu_memory_utilization: 0.85
      max_model_len: 8192
      max_num_seqs: 4
      enforce_eager: true
      trust_remote_code: true
      enable_prefix_caching: false
      max_num_batched_tokens: 32768
      engine_output_type: latent
    engine_input_source: [0]
    custom_process_input_func: vllm_omni.model_executor.stage_input_processors.qwen2_5_omni.thinker2talker
    default_sampling_params:
      temperature: 0.9
      top_p: 0.8
      top_k: 40
      max_tokens: 2048
      seed: 42
      detokenize: True
      repetition_penalty: 1.05
      stop_token_ids: [8294]

  - stage_id: 2
    stage_type: llm
    runtime:
      process: true
      devices: "0"
      max_batch_size: 4
    engine_args:
      model_stage: code2wav
      model_arch: Qwen2_5OmniForConditionalGeneration
      worker_cls: vllm_omni.worker.gpu_generation_worker.GPUGenerationWorker
      scheduler_cls: vllm_omni.core.sched.omni_generation_scheduler.OmniGenerationScheduler
      gpu_memory_utilization: 0.10
      max_num_seqs: 4
      enforce_eager: true
      trust_remote_code: true
      enable_prefix_caching: false
      max_num_batched_tokens: 32768
      engine_output_type: audio
    engine_input_source: [1]
    final_output: true
    final_output_type: audio
    default_sampling_params:
      temperature: 0.0
      top_p: 1.0
      top_k: -1
      max_tokens: 2048
      seed: 42
      detokenize: True
      repetition_penalty: 1.1

runtime:
  enabled: true
  defaults:
    window_size: -1
    max_inflight: 4
  edges:
    - from: 0
      to: 1
      window_size: -1
    - from: 1
      to: 2
      window_size: -1

# SPDX-License-Identifier: Apache-2.0
# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
"""
This example shows how to use vLLM for running offline inference
with the correct prompt format on Qwen3-Omni (thinker only).
"""

import os
from pathlib import Path
from typing import NamedTuple, Optional

import librosa
import numpy as np
import soundfile as sf
import torch.profiler
from PIL import Image
from vllm import SamplingParams
from vllm.assets.audio import AudioAsset
from vllm.assets.image import ImageAsset
from vllm.assets.video import VideoAsset, video_to_ndarrays
from vllm.multimodal.image import convert_image_mode
from vllm.utils import FlexibleArgumentParser

from vllm_omni.entrypoints.omni import Omni

SEED = 42


class QueryResult(NamedTuple):
    inputs: dict
    limit_mm_per_prompt: dict[str, int]


# NOTE: The default `max_num_seqs` and `max_model_len` may result in OOM on
# lower-end GPUs.
# Unless specified, these settings have been tested to work on a single L4.

default_system = (
    "You are Qwen, a virtual human developed by the Qwen Team, Alibaba "
    "Group, capable of perceiving auditory and visual inputs, as well as "
    "generating text and speech."
)


def get_text_query(question: str = None) -> QueryResult:
    if question is None:
        question = "Explain the system architecture for a scalable audio generation pipeline. Answer in 15 words."
    prompt = (
        f"<|im_start|>system\n{default_system}<|im_end|>\n"
        "<|im_start|>user\n"
        f"{question}<|im_end|>\n"
        f"<|im_start|>assistant\n"
    )
    return QueryResult(
        inputs={
            "prompt": prompt,
        },
        limit_mm_per_prompt={},
    )


def get_video_query(question: str = None, video_path: Optional[str] = None, num_frames: int = 16) -> QueryResult:
    if question is None:
        question = "Why is this video funny?"
    prompt = (
        f"<|im_start|>system\n{default_system}<|im_end|>\n"
        "<|im_start|>user\n<|vision_start|><|video_pad|><|vision_end|>"
        f"{question}<|im_end|>\n"
        f"<|im_start|>assistant\n"
    )

    if video_path:
        if not os.path.exists(video_path):
            raise FileNotFoundError(f"Video file not found: {video_path}")
        video_frames = video_to_ndarrays(video_path, num_frames=num_frames)
    else:
        video_frames = VideoAsset(name="baby_reading", num_frames=num_frames).np_ndarrays

    return QueryResult(
        inputs={
            "prompt": prompt,
            "multi_modal_data": {
                "video": video_frames,
            },
        },
        limit_mm_per_prompt={"video": 1},
    )


def get_image_query(question: str = None, image_path: Optional[str] = None) -> QueryResult:
    if question is None:
        question = "What is the content of this image?"
    prompt = (
        f"<|im_start|>system\n{default_system}<|im_end|>\n"
        "<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>"
        f"{question}<|im_end|>\n"
        f"<|im_start|>assistant\n"
    )

    if image_path:
        if not os.path.exists(image_path):
            raise FileNotFoundError(f"Image file not found: {image_path}")
        pil_image = Image.open(image_path)
        image_data = convert_image_mode(pil_image, "RGB")
    else:
        image_data = convert_image_mode(ImageAsset("cherry_blossom").pil_image, "RGB")

    return QueryResult(
        inputs={
            "prompt": prompt,
            "multi_modal_data": {
                "image": image_data,
            },
        },
        limit_mm_per_prompt={"image": 1},
    )


def get_audio_query(question: str = None, audio_path: Optional[str] = None, sampling_rate: int = 16000) -> QueryResult:
    if question is None:
        question = "What is the content of this audio?"
    prompt = (
        f"<|im_start|>system\n{default_system}<|im_end|>\n"
        "<|im_start|>user\n<|audio_start|><|audio_pad|><|audio_end|>"
        f"{question}<|im_end|>\n"
        f"<|im_start|>assistant\n"
    )

    if audio_path:
        if not os.path.exists(audio_path):
            raise FileNotFoundError(f"Audio file not found: {audio_path}")
        audio_signal, sr = librosa.load(audio_path, sr=sampling_rate)
        audio_data = (audio_signal.astype(np.float32), sr)
    else:
        audio_data = AudioAsset("mary_had_lamb").audio_and_sample_rate

    return QueryResult(
        inputs={
            "prompt": prompt,
            "multi_modal_data": {
                "audio": audio_data,
            },
        },
        limit_mm_per_prompt={"audio": 1},
    )


query_map = {
    "text": get_text_query,
    "use_audio": get_audio_query,
    "use_image": get_image_query,
    "use_video": get_video_query,
}


def _process_profiler_results(
    profiler: torch.profiler.profile,
    profiler_output_dir: Optional[str],
    request_id: int = 0,
) -> dict:
    """
    Process PyTorch profiler results and extract key metrics.

    Args:
        profiler: PyTorch profiler instance
        profiler_output_dir: Directory to save profiler traces
        request_id: Request identifier for trace file naming

    Returns:
        Dictionary with profiler statistics
    """
    trace_file = None
    try:
        # Export to Chrome trace format
        if profiler_output_dir:
            output_dir = Path(profiler_output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)

            trace_file = output_dir / f"trace_request_{request_id:05d}.json"
            profiler.export_chrome_trace(str(trace_file))
            print(f"  Profiler trace saved to: {trace_file}")

        # Get key statistics
        key_averages = profiler.key_averages()

        # Extract CUDA kernel statistics
        cuda_events = []
        total_cuda_time = 0
        total_cpu_time = 0

        for event in key_averages:
            if event.key.startswith("cuda"):
                # FunctionEventAvg has: cuda_time, cuda_time_total, cpu_time, cpu_time_total, count
                # cuda_time is average per call (likely self time), cuda_time_total includes children
                # To get total self time: multiply average by count
                avg_cuda_time = getattr(event, "cuda_time", 0) or 0
                avg_cpu_time = getattr(event, "cpu_time", 0) or 0
                event_count = getattr(event, "count", 1)

                # Total self time = average per call * number of calls
                cuda_time = (avg_cuda_time * event_count) / 1000.0  # Convert to ms
                cpu_time = (avg_cpu_time * event_count) / 1000.0
                total_cuda_time += cuda_time
                total_cpu_time += cpu_time

                cuda_events.append(
                    {
                        "name": event.key,
                        "cuda_time_ms": cuda_time,
                        "cpu_time_ms": cpu_time,
                        "count": getattr(event, "count", 0),
                    }
                )

        # Sort by CUDA time (descending)
        cuda_events.sort(key=lambda x: x["cuda_time_ms"], reverse=True)

        # Get top 10 most time-consuming operations
        top_ops = cuda_events[:10]

        result = {
            "total_cuda_time_ms": total_cuda_time,
            "total_cpu_time_ms": total_cpu_time,
            "top_operations": [
                {
                    "name": op["name"],
                    "cuda_time_ms": op["cuda_time_ms"],
                    "cpu_time_ms": op["cpu_time_ms"],
                    "count": op["count"],
                }
                for op in top_ops
            ],
            "total_events": len(cuda_events),
            "trace_file": str(trace_file) if trace_file else None,
        }

        # Print summary
        print(f"\n{'='*60}")
        print("Profiler Summary")
        print(f"{'='*60}")
        print(f"Total CUDA time: {total_cuda_time:.2f}ms")
        print(f"Total CPU time: {total_cpu_time:.2f}ms")
        print(f"Total events: {len(cuda_events)}")
        if top_ops:
            print(f"\nTop 5 operations by CUDA time:")
            for i, op in enumerate(top_ops[:5], 1):
                print(f"  {i}. {op['name']}: {op['cuda_time_ms']:.2f}ms (count: {op['count']})")
        print(f"{'='*60}\n")

        return result

    except Exception as e:
        print(f"[Warn] Error processing profiler results: {e}")
        return {"error": str(e), "trace_file": str(trace_file) if trace_file else None}


def main(args):
    model_name = "Qwen/Qwen3-Omni-30B-A3B-Instruct"

    # Get paths from args
    video_path = getattr(args, "video_path", None)
    image_path = getattr(args, "image_path", None)
    audio_path = getattr(args, "audio_path", None)

    # Get the query function and call it with appropriate parameters
    query_func = query_map[args.query_type]
    if args.query_type == "use_video":
        query_result = query_func(video_path=video_path, num_frames=getattr(args, "num_frames", 16))
    elif args.query_type == "use_image":
        query_result = query_func(image_path=image_path)
    elif args.query_type == "use_audio":
        query_result = query_func(audio_path=audio_path, sampling_rate=getattr(args, "sampling_rate", 16000))
    else:
        query_result = query_func()

    omni_llm = Omni(
        model=model_name,
        stage_configs_path=args.stage_configs_path,
        log_stats=args.enable_stats,
        log_file=("omni_llm_pipeline.log" if args.enable_stats else None),
        init_sleep_seconds=getattr(args, "init_sleep_seconds", 20),
        batch_timeout=getattr(args, "batch_timeout", 5),
        init_timeout=getattr(args, "init_timeout", 300),
        shm_threshold_bytes=getattr(args, "shm_threshold_bytes", 65536),
    )

    thinker_sampling_params = SamplingParams(
        temperature=0.4,
        top_p=0.9,
        top_k=-1,
        max_tokens=1200,
        repetition_penalty=1.05,
        logit_bias={},
        seed=SEED,
    )

    talker_sampling_params = SamplingParams(
        temperature=0.9,
        top_k=50,
        max_tokens=4096,
        seed=SEED,
        detokenize=False,
        repetition_penalty=1.05,
        stop_token_ids=[2150],  # TALKER_CODEC_EOS_TOKEN_ID
    )

    # Sampling parameters for Code2Wav stage (audio generation)
    code2wav_sampling_params = SamplingParams(
        temperature=0.0,
        top_p=1.0,
        top_k=-1,
        max_tokens=4096 * 16,
        seed=SEED,
        detokenize=True,
        repetition_penalty=1.1,
    )

    sampling_params_list = [
        thinker_sampling_params,
        talker_sampling_params,  # code predictor is integrated into talker for Qwen3 Omni
        code2wav_sampling_params,
    ]

    if args.txt_prompts is None:
        prompts = [query_result.inputs for _ in range(args.num_prompts)]
    else:
        assert args.query_type == "text", "txt-prompts is only supported for text query type"
        with open(args.txt_prompts, encoding="utf-8") as f:
            lines = [ln.strip() for ln in f.readlines()]
            prompts = [get_text_query(ln).inputs for ln in lines if ln != ""]
            print(f"[Info] Loaded {len(prompts)} prompts from {args.txt_prompts}")

    # Setup profiler if enabled
    if args.enable_profiler:
        if not args.profiler_output_dir:
            raise ValueError("--profiler-output-dir is required when --enable-profiler is set")
        profiler = torch.profiler.profile(
            activities=[
                torch.profiler.ProfilerActivity.CPU,
                torch.profiler.ProfilerActivity.CUDA,
            ],
            schedule=torch.profiler.schedule(wait=0, warmup=0, active=1, repeat=1),
            on_trace_ready=None,
            record_shapes=True,
            profile_memory=True,
            with_stack=True,
            with_flops=True,  # Enable FLOP counting (limited coverage: mainly matmul/conv)
        )
        profiler.start()
    else:
        profiler = None

    omni_outputs = omni_llm.generate(prompts, sampling_params_list)

    # Stop profiler and collect results
    if profiler:
        profiler.stop()
        # Process profiler results (single trace for all requests)
        _process_profiler_results(profiler, args.profiler_output_dir, 0)
    # Determine output directory: prefer --output-dir; fallback to --output-wav
    output_dir = args.output_dir if getattr(args, "output_dir", None) else args.output_wav
    os.makedirs(output_dir, exist_ok=True)

    for stage_outputs in omni_outputs:
        if stage_outputs.final_output_type == "text":
            for output in stage_outputs.request_output:
                request_id = int(output.request_id)
                text_output = output.outputs[0].text
                # Save aligned text file per request
                prompt_text = prompts[request_id]["prompt"]
                out_txt = os.path.join(output_dir, f"{request_id:05d}.txt")
                lines = []
                lines.append("Prompt:\n")
                lines.append(str(prompt_text) + "\n")
                lines.append("vllm_text_output:\n")
                lines.append(str(text_output).strip() + "\n")
                try:
                    with open(out_txt, "w", encoding="utf-8") as f:
                        f.writelines(lines)
                except Exception as e:
                    print(f"[Warn] Failed writing text file {out_txt}: {e}")
                print(f"Request ID: {request_id}, Text saved to {out_txt}")
        elif stage_outputs.final_output_type == "audio":
            for output in stage_outputs.request_output:
                request_id = int(output.request_id)
                audio_tensor = output.multimodal_output["audio"]
                output_wav = os.path.join(output_dir, f"output_{output.request_id}.wav")

                # Convert to numpy array and ensure correct format
                audio_numpy = audio_tensor.float().detach().cpu().numpy()

                # Ensure audio is 1D (flatten if needed)
                if audio_numpy.ndim > 1:
                    audio_numpy = audio_numpy.flatten()

                # Save audio file with explicit WAV format
                sf.write(output_wav, audio_numpy, samplerate=24000, format="WAV")
                print(f"Request ID: {request_id}, Saved audio to {output_wav}")


def parse_args():
    parser = FlexibleArgumentParser(description="Demo on using vLLM for offline inference with audio language models")
    parser.add_argument(
        "--query-type",
        "-q",
        type=str,
        default="mixed_modalities",
        choices=query_map.keys(),
        help="Query type.",
    )
    parser.add_argument(
        "--enable-stats",
        action="store_true",
        default=False,
        help="Enable writing detailed statistics (default: disabled)",
    )
    parser.add_argument(
        "--enable-profiler",
        action="store_true",
        default=False,
        help="Enable PyTorch profiler for detailed CPU/CUDA profiling (default: disabled)",
    )
    parser.add_argument(
        "--profiler-output-dir",
        type=str,
        default=None,
        help="Directory to save PyTorch profiler traces (required if --enable-profiler is set)",
    )
    parser.add_argument(
        "--init-sleep-seconds",
        type=int,
        default=20,
        help="Sleep seconds after starting each stage process to allow initialization (default: 20)",
    )
    parser.add_argument(
        "--batch-timeout",
        type=int,
        default=5,
        help="Timeout for batching in seconds (default: 5)",
    )
    parser.add_argument(
        "--init-timeout",
        type=int,
        default=300,
        help="Timeout for initializing stages in seconds (default: 300)",
    )
    parser.add_argument(
        "--shm-threshold-bytes",
        type=int,
        default=65536,
        help="Threshold for using shared memory in bytes (default: 65536)",
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default=None,
        help="Output directory for text and audio files (default: uses --output-wav).",
    )
    parser.add_argument(
        "--output-wav",
        default="output_audio",
        help="[Deprecated] Output wav directory (use --output-dir).",
    )
    parser.add_argument(
        "--num-prompts",
        type=int,
        default=1,
        help="Number of prompts to generate.",
    )
    parser.add_argument(
        "--txt-prompts",
        type=str,
        default=None,
        help="Path to a .txt file with one prompt per line (preferred).",
    )
    parser.add_argument(
        "--stage-configs-path",
        type=str,
        default=None,
        help="Path to a stage configs file.",
    )
    parser.add_argument(
        "--video-path",
        "-v",
        type=str,
        default=None,
        help="Path to local video file. If not provided, uses default video asset.",
    )
    parser.add_argument(
        "--image-path",
        "-i",
        type=str,
        default=None,
        help="Path to local image file. If not provided, uses default image asset.",
    )
    parser.add_argument(
        "--audio-path",
        "-a",
        type=str,
        default=None,
        help="Path to local audio file. If not provided, uses default audio asset.",
    )
    parser.add_argument(
        "--num-frames",
        type=int,
        default=16,
        help="Number of frames to extract from video (default: 16).",
    )
    parser.add_argument(
        "--sampling-rate",
        type=int,
        default=16000,
        help="Sampling rate for audio loading (default: 16000).",
    )

    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()
    main(args)
